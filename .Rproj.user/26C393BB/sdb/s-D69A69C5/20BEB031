{
    "contents" : "#APPLIED\n#Preset\nlibrary(ISLR)\nfix(Auto)\n\n\n##8. This question involves the use of simple linear regression on the Auto\n##data set.\n##(a) Use the lm() function to perform a simple linear regression with\n##mpg as the response and horsepower as the predictor. Use the\n##summary() function to print the results. Comment on the output.\nlm_q8<-lm(mpg~horsepower,data=Auto)\nsummary(lm_q8)\n#The p-value for the t value it's really close to 0 and the F-statistic is far\n#greater than one, we can reject the hipothesis of the mpg being independent \n#from the horsepower.\n#Nevertheless, the low value for the estimate (-0.16) indicates that the impact\n#per unit of Horsepower is not so big.\n#Also the value for the R squared is quite high, meaning more than half of the\n#variance is explained by this predictor (on training set, of course).\n##i. Is there a relationship between the predictor and the response?\n#A: yes (see comments on F-statistic an p-value)\n##ii. How strong is the relationship between the predictor and\n##the response?\n#A: Not VERY strong, the mpg decreases 0.15 units per unit increase in hp (horsepower)\n##iii. Is the relationship between the predictor and the response\n##positive or negative?\n#A: As commented before, negative.\n##iv. What is the predicted mpg associated with a horsepower of\n##98? What are the associated 95% confidence and prediction\n##intervals?\nattach(Auto)\npredict(lm_q8,data.frame(horsepower=98),interval=\"confidence\")\npredict(lm_q8,data.frame(horsepower=98),interval=\"prediction\")\n\n##(b) Plot the response and the predictor. Use the abline() function\n##to display the least squares regression line.\nplot(horsepower,mpg)\nabline(lm_q8,col=\"red\",lw=\"4\")\n##(c) Use the plot() function to produce diagnostic plots of the least\n##squares regression fit. Comment on any problems you see with\n##the fit.\npar(mfrow = c(2,2))\nplot(lm_q8)\n#Residual vs. Fitted shows some pattern, meaning that the data could be non lineal.\n#There are many points with high leverage an a couple of outliers\npar(mfrow = c(1,1))\n\n##9. This question involves the use of multiple linear regression on the\n##Auto data set.\n##(a) Produce a scatterplot matrix which includes all of the variables\n##in the data set.\npairs(Auto)\n##(b) Compute the matrix of correlations between the variables using\n##the function cor(). You will need to exclude the name variable,\n##which is qualitative.\ncor(subset(Auto,select = -name))\n##(c) Use the lm() function to perform a multiple linear regression\n##with mpg as the response and all other variables except name as\n##the predictors. Use the summary() function to print the results.\nlm_q9<-lm(mpg~.-name,data=Auto)\nsummary(lm_q9)\n##Comment on the output. For instance:\n##i. Is there a relationship between the predictors and the response?\n#F statistic (far from 1) shows that the predictor and the response are not\n#independent.\n##ii. Which predictors appear to have a statistically significant\n##relationship to the response?\n#Some of the predictors seem to be statistically significant enough\n#for the response to be dependent on them (weight,year & Origin, less significant,\n#but still enough is displacement)\n##iii. What does the coefficient for the year variable suggest?\n#The higher the year, the higher the mpg, with the second greatest slope of the\n#predictors, meaning it has one of the biggest impact in mpg.\n##(d) Use the plot() function to produce diagnostic plots of the linear\n##regression fit. Comment on any problems you see with the fit.\n##Do the residual plots suggest any unusually large outliers? Does\n##the leverage plot identify any observations with unusually high\n##leverage?\npar(mfrow = c(2,2))\nplot(lm_q9)\n#observation 14 have VERY inusual high leverage, (not high residual anyway)\npar(mfrow=c(1,1))\nplot(predict(lm_q9),rstudent(lm_q9))\n#Outliers over a value of rstudentized of > 3\n\n##(e) Use the * and : symbols to fit linear regression models with\n##interaction effects. Do any interactions appear to be statistically\n##significant?\nsink(\"outputMLR.txt\")\nsummary(lm(mpg~(.-name)*(.-name),data=Auto))\nsink()\n#Most significant is acceleration:origin, displacement:year, acceleration:year.\n#These below seem to be also quite significant statistically\n#Cylinders:year, Cylinders:acceleration, horsepower:acceleration, year:origin \n\n##(f) Try a few different transformations of the variables, such as\n##log(X), √X, X2. Comment on your findings.  \npairs(Auto)\n#some of the responses seem to have a radical form (exponent between 0 and 1)\n# for example displacement,horsepower, and weight, compared to mpg\nplot(mpg~horsepower, data=Auto)\nsummary(lm(mpg~horsepower),data=Auto)\nplot(mpg~I(horsepower^(1/4)), data=Auto)\nsummary(lm(mpg~I(horsepower^(1/4))))\n#We can see in the second example, the linear model explains more variance than\n#the first one, (adjusted R-Squared)  0.6049 vs 0.657\n####INTERESTING TODO: Create a function that calculates the optimal power for this model\n####not very afraid of overfit, but it can happen (be careful)\n\n##10. This question should be answered using the Carseats data set.\nlibrary(ISLR)\n##(a) Fit a multiple regression model to predict Sales using Price,\n##Urban, and US.\npairs(Carseats[,c(\"Sales\",\"Price\",\"Urban\",\"US\")])\nlm_q10<-lm(Sales~Price+Urban+US,data=Carseats)\nsummary(lm_q10)\n##(b) Provide an interpretation of each coefficient in the model. Be\n##careful—some of the variables in the model are qualitative!\n#The Price seem to be related with Sales, for each unit increase in Price there is\n#a 0.054 units decrease in sales. Sales seem to be independent from the Urban\n#predictor, with a VERY high p-value (meaning the value for Urban yes is likely\n#to happen by chance)\n#The Predictor for USYes indicates that there is a increase in 1.2 units in sales\n#when the Value for this qualitative variable is Yes.\n#The bad news is the R-squared value, under 0.25\n##(c) Write out the model in equation form, being careful to handle\n##the qualitative variables properly.\n#ý<-13.04+Price*(-0.05)+(Urban=\"Yes\")*(-0.02)+(US=\"Yes\")*(1.20)\n\n##(d) For which of the predictors can you reject the null hypothesis\n##H0 : βj = 0?\n#Just for Urban, as commented before.\n##(e) On the basis of your response to the previous question, fit a\n##smaller model that only uses the predictors for which there is\n##evidence of association with the outcome.\nlm_q10.2<-lm(Sales~Price+US,data=Carseats)\nsummary(lm_q10.2)\n##(f) How well do the models in (a) and (e) fit the data?\nanova(lm_q10,lm_q10.2)\n#R squared is around 0.23, meaning that less than 1 third of the varianze is\n#explained by the model.\n\n##(g) Using the model from (e), obtain 95% confidence intervals for\n##the coefficient(s).\nconfint(lm_q10.2,level=0.95)\n\n##(h) Is there evidence of outliers or high leverage observations in the\n##model from (e)?\npar(mfrow=c(2,2))\nplot(lm_q10.2)\npar(mfrow=c(1,1))\n#one or two outliers\nplot(predict(lm_q10.2),rstudent(lm_q10.2))\n#no outliers\n\n##11. In this problem we will investigate the t-statistic for the null hypothesis\n##H0 : β = 0 in simple linear regression without an intercept. To\n##begin, we generate a predictor x and a response y as follows.\n\n set.seed (1)\n x=rnorm (100)\n y=2*x+rnorm (100)\n\n##(a) Perform a simple linear regression of y onto x, without an intercept.\n##Report the coefficient estimate ˆβ, the standard error of\n##this coefficient estimate, and the t-statistic and p-value associated\n##with the null hypothesis H0 : β = 0. Comment on these\n##results. (You can perform regression without an intercept using\n##the command lm(y∼x+0).)\nlm_q11<-lm(y~x+0)\n#the slope is almost perfect, we gave a slope of 2 (plus noise) and we're getting 1.994\nsummary(lm_q11)\n##(b) Now perform a simple linear regression of x onto y without an\n##intercept, and report the coefficient estimate, its standard error,\n##and the corresponding t-statistic and p-values associated with\n##the null hypothesis H0 : β = 0. Comment on these results\nsummary(lm(x~y+0))\n#same t and p values\n\n##(c) What is the relationship between the results obtained in (a) and\n##(b)?\n#The're basically the same regression (rotated 90º), β_y = 1/β_x\n\n##(d) For the regression of Y onto X without an intercept, the tstatistic\n##for H0 : β = 0 takes the form ˆβ/SE( ˆ β), where ˆ β is\n##given by (3.38), and where\n##SE( ˆ β) = !\n##n\n##i=1(yi − xi ˆ β)2\n##(n − 1)\n##n\n##i=1 x2 i\n##.\n##124 3. Linear Regression\n##(These formulas are slightly different from those given in Sections\n##3.1.1 and 3.1.2, since here we are performing regression\n##without an intercept.) Show algebraically, and confirm numerically\n##in R, that the t-statistic can be written as\n##(√n − 1)\n##n\n##i=1 xiyi\n##(\n##n\n##i=1 x2i\n##)(\n##n\n##i=1 y2\n##i ) − (\n##n\n##i=1 xiyi )2\n##.\n#In the notebook\n##(e) Using the results from (d), argue that the t-statistic for the regression\n##of y onto x is the same as the t-statistic for the regression\n##of x onto y.\n#Exchangable variables in the formula\n##(f) In R, show that when regression is performed with an intercept,\n##the t-statistic for H0 : β1 = 0 is the same for the regression of y\n##onto x as it is for the regression of x onto y.\nsummary(lm(x~y))\nsummary(lm(y~x))\n\n##12. This problem involves simple linear regression without an intercept.\n##(a) Recall that the coefficient estimate ˆ β for the linear regression of\n##Y onto X without an intercept is given by (3.38). Under what\n##circumstance is the coefficient estimate for the regression of X\n##onto Y the same as the coefficient estimate for the regression of\n##Y onto X?\n#sum(x^2)==sum(y^2)\n##(b) Generate an example in R with n = 100 observations in which\n##the coefficient estimate for the regression of X onto Y is different\n##from the coefficient estimate for the regression of Y onto X.\nx=rnorm(100)\ny=x*0.5\nlm(x~y+0)\nlm(y~x+0)\n##(c) Generate an example in R with n = 100 observations in which\n##the coefficient estimate for the regression of X onto Y is the\n##same as the coefficient estimate for the regression of Y onto X.\nx=rnorm(100)\ny=(sample(x,100))\nlm(x~y+0)\nlm(y~x+0)\n\n##13. In this exercise you will create some simulated data and will fit simple\n##linear regression models to it. Make sure to use set.seed(1) prior to\n##starting part (a) to ensure consistent results.\nset.seed(1)\n##(a) Using the rnorm() function, create a vector, x, containing 100\n##observations drawn from a N(0, 1) distribution. This represents\n##a feature, X.\nx<-rnorm(100,0,1)\n##(b) Using the rnorm() function, create a vector, eps, containing 100\n##observations drawn from a N(0, 0.25) distribution i.e. a normal\n##distribution with mean zero and variance 0.25.\neps<-rnorm(100,0,sqrt(0.25))\n#careful with variance (sigma squared) and standard deviation (sigma)\n##(c) Using x and eps, generate a vector y according to the model\n##Y = −1 + 0.5X + . (3.39)\n##What is the length of the vector y? What are the values of β0\n##and β1 in this linear model?\ny<--1+(0.5*x)+eps\nlength(y)\nlm_q13<-lm(y~x)\nsummary(lm_q13)\n#-0.998, 0.48\n#quite approx.\n##(d) Create a scatterplot displaying the relationship between x and\n##y. Comment on what you observe.\n##(e) Fit a least squares linear model to predict y using x. Comment\n##on the model obtained. How do ˆ β0 and ˆ β1 compare to β0 and\n##β1?\n##(f) Display the least squares line on the scatterplot obtained in (d).\n##Draw the population regression line on the plot, in a different\n##color. Use the legend() command to create an appropriate legend.\n##(g) Now fit a polynomial regression model that predicts y using x\n##and x2. Is there evidence that the quadratic term improves the\n##model fit? Explain your answer.\n##(h) Repeat (a)–(f) after modifying the data generation process in\n##such a way that there is less noise in the data. The model (3.39)\n##should remain the same. You can do this by decreasing the variance\n##of the normal distribution used to generate the error term\n## in (b). Describe your results.\n##(i) Repeat (a)–(f) after modifying the data generation process in\n##such a way that there is more noise in the data. The model\n##(3.39) should remain the same. You can do this by increasing\n##the variance of the normal distribution used to generate the\n##error term  in (b). Describe your results.\n##(j) What are the confidence intervals for β0 and β1 based on the\n##original data set, the noisier data set, and the less noisy data\n##set? Comment on your results.\n\n##14. This problem focuses on the collinearity problem.\n##(a) Perform the following commands in R:\n##> set .seed (1)\n##> x1=runif (100)\n##> x2 =0.5* x1+rnorm (100) /10\n##> y=2+2* x1 +0.3* x2+rnorm (100)\n##The last line corresponds to creating a linear model in which y is\n##a function of x1 and x2. Write out the form of the linear model.\n##What are the regression coefficients?\n##(b) What is the correlation between x1 and x2? Create a scatterplot\n##displaying the relationship between the variables.\n##(c) Using this data, fit a least squares regression to predict y using\n##x1 and x2. Describe the results obtained. What are ˆ β0, ˆ β1, and\n##ˆ β2? How do these relate to the true β0, β1, and β2? Can you\n##reject the null hypothesis H0 : β1 = 0? How about the null\n##hypothesis H0 : β2 = 0?\n##126 3. Linear Regression\n##(d) Now fit a least squares regression to predict y using only x1.\n##Comment on your results. Can you reject the null hypothesis\n##H0 : β1 = 0?\n##(e) Now fit a least squares regression to predict y using only x2.\n##Comment on your results. Can you reject the null hypothesis\n##H0 : β1 = 0?\n##(f) Do the results obtained in (c)–(e) contradict each other? Explain\n##your answer.\n##(g) Now suppose we obtain one additional observation, which was\n##unfortunately mismeasured.\n##> x1=c(x1 , 0.1)\n##> x2=c(x2 , 0.8)\n##> y=c(y,6)\n##Re-fit the linear models from (c) to (e) using this new data. What\n##effect does this new observation have on the each of the models?\n##In each model, is this observation an outlier? A high-leverage\n##point? Both? Explain your answers.\n\n##15. This problem involves the Boston data set, which we saw in the lab\n##for this chapter. We will now try to predict per capita crime rate\n##using the other variables in this data set. In other words, per capita\n##crime rate is the response, and the other variables are the predictors.\n##(a) For each predictor, fit a simple linear regression model to predict\n##the response. Describe your results. In which of the models is\n##there a statistically significant association between the predictor\n##and the response? Create some plots to back up your assertions.\n##(b) Fit a multiple regression model to predict the response using\n##all of the predictors. Describe your results. For which predictors\n##can we reject the null hypothesis H0 : βj = 0?\n##(c) How do your results from (a) compare to your results from (b)?\n##Create a plot displaying the univariate regression coefficients\n##from (a) on the x-axis, and the multiple regression coefficients\n##from (b) on the y-axis. That is, each predictor is displayed as a\n##single point in the plot. Its coefficient in a simple linear regression\n##model is shown on the x-axis, and its coefficient estimate\n##in the multiple linear regression model is shown on the y-axis.\n##(d) Is there evidence of non-linear association between any of the\n##predictors and the response? To answer this question, for each\n##predictor X, fit a model of the form\n##Y = β0 + β1X + β2X2 + β3X3 + .\n",
    "created" : 1431282643062.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2347485864",
    "id" : "20BEB031",
    "lastKnownWriteTime" : 1431703173,
    "path" : "~/R Projects/ISLR/practica3.R",
    "project_path" : "practica3.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}